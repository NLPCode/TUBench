# TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions

> Large Vision-Language Models (LVLMs) have achieved remarkable progress on visual perception and linguistic interpretation. Despite their impressive capabilities across various tasks, LVLMs still suffer from the issue of hallucination, which involves generating content that is incorrect or unfaithful to the visual or textual inputs. Traditional benchmarks, such as MME and POPE, evaluate hallucination in LVLMs within the scope of visual question answering (VQA) using answerable questions. However, some questions are unanswerable due to insufficient information in the images, and the performance of LVLMs on such unanswerable questions remains underexplored. To fill in this research blank, we propose TUBench, a benchmark specifically designed to evaluate the reliability of LVLMs using unanswerable questions. TUBench comprises an extensive collection of high-quality, unanswerable questions that are meticulously crafted using ten distinct strategies. To thoroughly evaluate LVLMs, the unanswerable questions in TUBench use images from four diverse domains as visual contexts: screenshots of code snippets, natural images, geometry diagrams, and screenshots of statistical tables. These unanswerable questions are tailored to test LVLMs' trustworthiness in code reasoning, commonsense reasoning, geometric reasoning, and mathematical reasoning related to tables, respectively. We conducted a comprehensive quantitative evaluation of 28 leading foundational models on TUBench, with Gemini-1.5-Pro, the top-performing model, achieving an average accuracy of 69.2\%, and GPT-4o, the third-ranked model, reaching 66.7\% average accuracy, in determining whether questions are answerable. Furthermore, our manual analysis of the model outputs reveals that: (1) Gemini-1.5-Pro provides both correct answers and explanations in only 41\% of cases, and (2) hallucinations are the primary cause of error, accounting for 58.5\% of the incorrect explanations generated by Gemini-1.5-Pro. These findings highlight that TUBench presents a significant challenge to current LVLMs, and offers a new perspective for evaluating hallucinations and trustworthiness through the lens of unanswerable questions.

## Unanswerable Code Reasoning (UCR)
For UCR, we create unanswerable questions using the following three strategies:
- S.1. Introduce uncertainties into code screenshots by adding random functions.
- S.2. Introduce uncertainties into code screenshots by omitting variable initialization.
- S.3. Introduce uncertainties into code screenshots by deliberately leaving certain lines of code incomplete.

<img src="./images/ucr.png" width="100%" height="100%">

## Unanswerable Visual Question Answering (UVQA)
For UVQA, we create unanswerable questions using the following five strategies:

- S.4. The information required to answer the question is occluded in the image.
- S.5. The details necessary to answer the question are hard or impossible to discern.
- S.6. The required information is out of the picture frame.
- S.7. The spatial relationship is indeterminate.
- S.8. The required information is not indicated in the image.

<img src="./images/uvqa.png" width="100%" height="100%">


## Unanswerable GeoQA (UGeoQA)
For UGeoQA, we create unanswerable questions using the following strategy:
- S.9. To construct an unanswerable question, we deliberately remove a condition from the answerable question.

<img src="./images/ugeoqa.png" width="100%" height="100%">

## Unanswerable UTabMWP (UTabMWP)
For UTabMWP, we create unanswerable questions using the following strategy:
- S.10. To render the original question unanswerable, we deliberately occlude crucial information in the left image, thus creating the altered image displayed on the right.

<img src="./images/utabmwp.png" width="100%" height="100%">
